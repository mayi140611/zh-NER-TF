{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# %load main.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, argparse, time, random\n",
    "from utils import str2bool, get_logger, get_entity\n",
    "from data import read_corpus, read_dictionary, tag2label, random_embedding\n",
    "\n",
    "from model import BiLSTM_CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Session configuration\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # default: 0\n",
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.2  # need ~700MB GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train_data TRAIN_DATA]\n",
      "                             [--test_data TEST_DATA] [--batch_size BATCH_SIZE]\n",
      "                             [--epoch EPOCH] [--hidden_dim HIDDEN_DIM]\n",
      "                             [--optimizer OPTIMIZER] [--CRF CRF] [--lr LR]\n",
      "                             [--clip CLIP] [--dropout DROPOUT]\n",
      "                             [--update_embedding UPDATE_EMBEDDING]\n",
      "                             [--pretrain_embedding PRETRAIN_EMBEDDING]\n",
      "                             [--embedding_dim EMBEDDING_DIM]\n",
      "                             [--shuffle SHUFFLE] [--mode MODE]\n",
      "                             [--demo_model DEMO_MODEL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Ian\\AppData\\Roaming\\jupyter\\runtime\\kernel-1031f66c-1f35-4276-8431-9d962fca18db.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "## hyperparameters\n",
    "parser = argparse.ArgumentParser(description='BiLSTM-CRF for Chinese NER task')\n",
    "parser.add_argument('--train_data', type=str, default='data_path', help='train data source')\n",
    "parser.add_argument('--test_data', type=str, default='data_path', help='test data source')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='#sample of each minibatch')\n",
    "parser.add_argument('--epoch', type=int, default=40, help='#epoch of training')\n",
    "parser.add_argument('--hidden_dim', type=int, default=300, help='#dim of hidden state')\n",
    "parser.add_argument('--optimizer', type=str, default='Adam', help='Adam/Adadelta/Adagrad/RMSProp/Momentum/SGD')\n",
    "parser.add_argument('--CRF', type=str2bool, default=True, help='use CRF at the top layer. if False, use Softmax')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--clip', type=float, default=5.0, help='gradient clipping')\n",
    "parser.add_argument('--dropout', type=float, default=0.5, help='dropout keep_prob')\n",
    "parser.add_argument('--update_embedding', type=str2bool, default=True, help='update embedding during training')\n",
    "parser.add_argument('--pretrain_embedding', type=str, default='random', help='use pretrained char embedding or init it randomly')\n",
    "parser.add_argument('--embedding_dim', type=int, default=300, help='random init char embedding_dim')\n",
    "parser.add_argument('--shuffle', type=str2bool, default=True, help='shuffle training data before each epoch')\n",
    "parser.add_argument('--mode', type=str, default='demo', help='train/test/demo')\n",
    "parser.add_argument('--demo_model', type=str, default='1521112368', help='model for test and demo')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 3905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get char embeddings\n",
    "word2id = read_dictionary(os.path.join('.', 'data_path', 'word2id.pkl'))\n",
    "type(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '风', '时', '今', '天', '有', '收', '藏', '价', '值', '书', '你', '没', '买', '明', '日', '就', '叫', '悔', '不', '初', '！', '本', '是', '所', '传', '统', '门', '类', '中', '第', '一', '大', '户', '只', '我', '们', '结', '束', '温', '饱', '间', '太', '短', '而', '已', '。', '因', '关', '寇', '在', '京', '掠', '夺', '文', '物', '详', '情', '界', '较', '为', '重', '视', '也', '北', '史', '料', '要', '件', '之', '册', '<NUM>', '年', '月', '油', '印', '《', '保', '存', '管', '状', '态', '调', '查', '报', '告', '》', '范', '围', '涉', '及', '故', '宫', '、', '历', '博', '古', '研', '清', '华', '图', '馆', '伪', '资', '库', '等', '二', '十', '几', '家', '言', '以', '上', '洋', '三', '余', '珍', '贵', '乡', '献', '特', '定', '期', '刊', '某', '名', '或', '著', '多', '种', '出', '版', '专', '题', '注', '意', '精', '品', '非', '卖', '纪', '念', '集', '系', '列', '那', '过', '经', '够', '您', '玩', '味', '无', '穷', '了', '受', '到', '郑', '振', '铎', '先', '生', '阿', '英', '作', '启', '示', '从', '个', '人', '条', '发', '瞄', '准', '现', '代', '究', '空', '白', '点', '解', '放', '区', '民', '党', '毁', '禁', '靠', '自', '己', '鉴', '赏', '能', '力', '将', '尽', '可', '籍', '汇', '身', '旁', '并', '得', '认', '真', '正', '义', '稀', '见', '度', '才', '质', '量', '核', '心', '数', '少', '反', '映', '更', '薪', '层', '承', '浩', '如', '烟', '海', '与', '财', '广', '典', '怡', '雅', '好', '立', '说', '这', '全', '部', '闲', '钱', '地', '方', '夫', '固', '但', '化', '底', '蕴', '对', '于', '达', '美', '感', '和', '趣', '业', '背', '景', '下', '显', '增', '致', '富', '法', '敢', '议', '听', '哪', '位', '否', '则', '吃', '喝', '许', '入', '此', '道', '朋', '友', '叹', '惜', '赶', '开', '始', '五', '角', '线', '装', '近', '热', '极', '带', '动', '各', '格', '去', '又', '被', '评', '“', '市', '首', '届', '庭', '元', '星', '”', '姜', '德', '话', '林', '枝', '叶', '爱', '细', '介', '绍', '妇', '口', '乐', '贫', '逸', '闻', '事', '每', '接', '外', '归', '员', '至', '食', '谱', '欣', '喜', '若', '狂', '；', '交', '换', '离', '虽', '复', '李', '后', '主', '挥', '泪', '别', '娥', '变', '会', '缘', '把', '欧', '港', '台', '流', '行', '画', '具', '堂', '奇', '昂', '跟', '踪', '际', '最', '新', '艺', '趋', '势', '搜', '提', '高', '技', '捷', '径', '俩', '都', '产', '营', '销', '夜', '深', '静', '土', '纸', '毛', '边', '久', '战', '火', '幕', '歌', '泣', '血', '雨', '腥', '悲', '壮', '场', '便', '目', '怎', '催', '奋', '进', '次', '老', '革', '命', '签', '字', '再', '讲', '述', '斗', '争', '叮', '嘱', '继', '续', '机', '向', '社', '展', '青', '育', '红', '军', '领', '孙', '毅', '（', '—', '）', '晋', '察', '冀', '抗', '根', '据', '盖', '章', '榜', '相', '赠', '王', '汉', '延', '安', '烈', '张', '雷', '女', '四', '士', '西', '蕾', '六', '前', '学', '院', '校', '录', '写', '父', '永', '垂', '朽', '级', '未', '由', '频', '繁', '艰', '苦', '遭', '世', '购', '易', '尤', '周', '恩', '总', '理', '马', '骏', '南', '同', '钊', '遗', '篇', '孤', '术', '演', '黄', '埔', '政', '治', '宣', '材', '译', '阶', '央', '共', '称', '两', '完', '整', '建', '播', '敌', '占', '：', '确', '弃', '取', '用', '合', '其', '逾', '千', '跨', '越', '加', '份', '婚', '利', '均', '功', '岗', '敲', '打', '着', '谐', '节', '拍', '读', '却', '活', '旋', '律', '失', '插', '队', '待', '徒', '考', '春', '韵', '实', '果', '让', '勇', '锐', '任', '既', '他', '诗', '付', '梓', '序', '样', '兼', '者', '敬', '亦', '亲', '钦', '想', '指', '侧', '论', '弥', '神', '龙', '尾', '澈', '坦', '荡', '瑕', '呢', '？', '还', '茂', '峻', '岭', '山', '瀑', '匠', '胸', '揽', '丘', '壑', '境', '遐', '思', '通', '契', '恐', '谛', '岂', '止', '骋', '笔', '泼', '墨', '摹', '略', '概', '谓', '木', '柯', '荣', '句', '观', '令', '驰', '乍', '暖', '媚', '树', '旺', '笋', '乔', '参', '绿', '犹', '嫣', '…', '皆', '宋', '娟', '秀', '蒙', '泽', '润', '往', '知', '济', '兄', '置', '良', '聚', '辄', '丹', '遁', '辞', '殊', '素', '露', '瞻', '恕', '弟', '胆', '苟', '善', '矣', '唐', '难', '智', '耶', '惟', '倒', '宿', '儒', '气', '息', '妙', '痕', '迹', '罢', '里', '恭', '罪', '尊', '诤', '曰', '处', '公', '何', '尝', '跳', '篱', '隶', '直', '季', '什', '么', '逊', '足', '求', '远', '原', '色', '啧', '桂', '冠', '奉', '札', '获', '使', '转', '翰', '采', '盈', '馨', '尚', '积', '习', '刻', '雕', '琢', '旧', '绳', '觉', '嘛', '辅', '仁', '步', '晤', '绝', '识', '即', '早', '缓', '淡', '稍', '乃', '幽', '邃', '咀', '嚼', '甘', '滋', '泛', '慕', '升', '仰', '织', '欢', '她', '卫', '庄', '严', '圣', '斯', '阔', '酸', '腐', '攀', '词', '些', '很', '竟', '朝', '夕', '形', '陌', '路', '旦', '分', '手', '记', '忆', '常', '楚', '玉', '峰', '茶', '陈', '师', '搬', '楼', '座', '荒', '废', '恢', '昔', '且', '比', '派', '信', '包', '园', '声', '农', '毕', '浪', '涯', '回', '走', '寄', '偏', '僻', '困', '室', '破', '庙', '暴', '午', '倾', '塌', '封', '修', '闭', '胜', '授', '陆', '充', '满', '逐', '渐', '冷', '落', '舍', '小', '描', '灵', '性', '应', '邀', '梁', '秋', '坐', '滑', '竿', '八', '茗', '临', '窗', '钓', '潭', '鲤', '鱼', '给', '傅', '做', '河', '水', '煮', '迎', '墙', '幅', '劲', '泉', '香', '渴', '候', '坡', '简', '陋', '氛', '副', '联', '烹', '宜', '竹', '翻', '羽', '卢', '片', '涛', '野', '光', '陶', '醉', '面', '阡', '纵', '横', '稻', '花', '飘', '牧', '笛', '悠', '株', '抱', '榕', '挡', '住', '凉', '拂', '炎', '盛', '夏', '森', '袭', '吊', '脚', '栋', '悬', '崖', '妈', '送', '懂', '体', '缺', '伴', '随', '岁', '躲', '避', '轰', '炸', '疏', '散', '诉', '粮', '备', '掏', '轻', '累', '饿', '戴', '眼', '镜', '头', '扣', '顶', '绒', '帽', '拄', '拐', '杖', '灾', '街', '巷', '问', '津', '炮', '制', '庆', '饥', '城', '畅', '饮', '缕', '愁', '溢', '怀', '仿', '佛', '甜', '辣', '顿', '爽', '尘', '寞', '沏', '杯', '芽', '嫩', '染', '内', '摘', '泡', '吹', '绽', '刚', '迢', '云', '雾', '郊', '麓', '拿', '语', '车', '忘', '左', '握', '盘', '腾', '右', '摸', '东', '扔', '啊', '司', '嘴', '脸', '笑', '容', '拉', '驾', '驶', '释', '遇', '熟', '耽', '搁', '摇', '颗', '蓬', '乱', '草', '似', '胖', '虾', '般', '凸', '睛', '算', '找', '磨', '爷', '块', '‘', '腰', '子', '’', '吧', '恼', '辆', '嘎', '停', '茫', '仇', '恨', '浇', '灌', '漠', '龟', '裂', '田', '缩', '猜', '疑', '距', '愿', '辜', '负', '鞋', '瞅', '哼', '啦', '急', '跑', '影', '逢', '侃', '溜', '供', '职', '珠', '宝', '饰', '杂', '志', '巧', '组', '稿', '悉', '编', '辑', '拨', '蹦', '灯', '稳', '嫂', '递', '看', '甩', '硬', '邦', '顺', '帮', '孩', '袱', '仍', '阴', '谁', '欠', '探', '捎', '双', '式', '属', '商', '阳', '移', '履', '疲', '惫', '村', '耐', '沉', '闷', '厢', '异', '泻', '搭', '讪', '挣', '愧', '疚', '辛', '净', '站', '挂', '途', '办', '快', '班', '断', '索', '斩', '奏', '抢', '约', '像', '倦', '铺', '钻', '蹿', '哥', '阵', '昙', '燕', '族', '运', '担', '拒', '宴', '请', '祸', '焦', '访', '谈', '贪', '官', '污', '吏', '衙', '暗', '舆', '监', '督', '强', '宽', '慰', '坛', '仅', '惭', '耻', '辱', '县', '店', '支', '遥', '导', '警', '干', '鸣', '伸', '旗', '姓', '需', '祖', '宗', '礼', '糟', '粕', '侵', '犯', '维', '护', '服', '务', '七', '妃', '纳', '养', '炫', '耀', '男', '防', '仪', '举', '吐', '痰', '浓', '亨', '除', '登', '规', '忽', '仲', '贤', '予', '尧', '普', '兵', '权', '丁', '响', '婆', '款', '匹', '表', '居', '局', '免', '袁', '崇', '焕', '墓', '危', '排', '饭', '矫', '咱', '桌', '酒', '席', '激', '曾', '扶', '跌', '撞', '摔', '它', '谨', '谋', '构', '象', '差', '歪', '斜', '浅', '纯', '创', '伯', '持', '况', '搞', '压', '肩', '憾', '益', '绪', '减', '连', '棋', '咫', '尺', '号', '舞', '团', '答', '试', '音', '喇', '叭', '铅', '唱', '圆', '梦', '计', '施', '选', '喊', '偶', '脱', '晚', '湾', '俯', '峦', '拥', '翠', '渺', '卷', '徐', '裔', '韩', '石', '窟', '金', '矿', '川', '撰', '造', '囊', '括', '瞪', '呆', '证', '俗', '堪', '寻', '游', '客', '旅', '挤', '彩', '菲', '艳', '照', '依', '丰', '掘', '召', '筹', '赵', '凤', '洞', '僧', '龛', '基', '怒', '哀', '漫', '轮', '融', '母', '哲', '锁', '耗', '虚', '壁', '迦', '涅', '卧', '米', '半', '隐', '岩', '陷', '酷', '平', '齐', '损', '衣', '丽', '披', '凿', '绰', '颇', '秘', '亮', '秉', '烛', '电', '优', '肖', '引', '项', '杰', '九', '浴', '灰', '巨', '汩', '喷', '畔', '藤', '萝', '绕', '飞', '恍', '姿', '驻', '辉', '煌', '孔', '雀', '屏', '捏', '器', '塔', '房', '屈', '密', '麻', '零', '牛', '拜', '郎', '杨', '颂', '州', '赐', '紫', '袋', '勾', '互', '嬉', '戏', '憨', '睡', '憩', '丛', '颈', '蝶', '潜', '栩', '微', '彻', '谷', '斧', '乎', '耳', '苍', '绘', '留', '折', '伏', '惨', '碑', '载', '淳', '熙', '佑', '沿', '峭', '削', '迷', '凝', '宛', '纱', '遮', '幻', '含', '蹄', '敦', '冈', '摩', '寓', '焰', '玄', '洁', '射', '绚', '环', '纷', '呈', '魅', '慧', '预', '独', '赢', '设', '刷', '附', '盲', '页', '码', '效', '貌', '触', '凹', '剪', '贴', '另', '黑', '斑', '斓', '幸', '唯', '颜', '适', '费', '怪', '诞', '残', '疾', '伤', '害', '剧', '模', '限', '填', '补', '衷', '挑', '摆', '辩', '驳', '超', '聋', '哑', '曲', '型', '铁', '怕', '脉', '蚩', '帝', '寨', '裹', '挟', '鼓', '厮', '杀', '震', '撼', '雄', '宇', '祭', '埋', '端', '鹰', '涿', '鹿', '亢', '征', '恒', '托', '榆', '淌', '沧', '桑', '蔽', '宏', '枯', '倔', '股', '惧', '轩', '裸', '撕', '扯', '碎', '萌', '凄', '刀', '砍', '郁', '葱', '松', '鼠', '穿', '推', '追', '仗', '仓', '促', '夯', '坚', '窝', '紧', '躬', '丈', '筑', '厚', '决', '愈', '诚', '觅', '瞠', '低', '峙', '隔', '毗', '敞', '叠', '撒', '硕', '屯', '守', '揭', '跋', '悟', '窄', '陡', '蜿', '蜒', '踏', '浮', '塘', '戈', '扰', '浑', '浊', '切', '诸', '侯', '遂', '擒', '晨', '遏', '狼', '剽', '悍', '孝', '宾', '潮', '莫', '肥', '犬', '劳', '吗', '箱', '琴', '侍', '汽', '该', '乘', '妨', '例', '尖', '怜', '娇', '惯', '娃', '恶', '责', '吞', '剥', '案', '邻', '击', '凶', '兀', '骂', '猝', '涵', '乳', '臭', '赌', '瘾', '砝', '禅', '划', '策', '殷', '假', '鸡', '嘲', '弄', '慈', '祥', '惋', '赞', '扬', '必', '疼', '健', '赤', '伟', '绩', '牺', '牲', '甚', '姻', '幼', '布', '乞', '抚', '汁', '批', '弹', '沦', '凡', '省', '牢', '镌', '隆', '邓', '颖', '龄', '袖', '址', '罗', '呼', '酝', '酿', '旨', '灭', '怖', '抽', '输', '鲜', '奴', '训', '练', '仆', '寒', '遍', '忧', '涌', '病', '吼', '众', '醒', '昨', '勤', '俭', '死', '克', '弱', '患', '缠', '喉', '鞠', '肃', '鸦', '啸', '群', '堆', '闹', '哭', '晖', '沐', '迈', '殿', '诫', '蠢', '臂', '险', '按', '赔', '投', '符', '痛', '欲', '额', '偿', '厂', '操', '慎', '肘', '截', '莞', '凭', '借', '忐', '忑', '匆', '戚', '聪', '辈', '亏', '江', '瞒', '板', '郭', '判', '协', '乏', '妻', '审', '裁', '吵', '休', '忍', '矛', '盾', '医', '针', '灸', '诊', '癜', '刘', '疗', '忌', '羊', '肉', '<ENG>', '芝', '锻', '炼', '皮', '涂', '药', '消', '丸', '咨', '询', '段', '祛', '湿', '络', '辨', '疫', '改', '液', '循', '肤', '末', '痒', '症', '碍', '躯', '殖', '单', '岛', '床', '鳞', '屑', '肝', '泌', '紊', '谢', '障', '胞', '坏', '％', '摒', '诱', '惑', '择', '享', '沓', '槽', '悄', '逝', '闯', '辙', '譬', '羡', '贾', '番', '奥', '衡', '违', '滚', '洪', '锢', '缚', '沮', '丧', '寂', '企', '嫌', '招', '账', '谙', '掉', '伙', '奶', '榨', '豆', '浆', '执', '柔', '畜', '武', '攻', '苏', '康', '努', '诺', '窍', '昼', '退', '矩', '馒', '餐', '碗', '菜', '苛', '慷', '慨', '捐', '誓', '叔', '兰', '厨', '培', '牌', '挚', '佳', '廉', '肴', '誉', '抓', '肯', '终', '懒', '降', '票', '栽', '刺', '陪', '蒜', '孺', '砖', '卡', '氧', '替', '屋', '私', '裕', '锦', '添', '球', '井', '措', '喻', '误', '励', '讼', '彼', '虑', '卑', '鬼', '傲', '骨', '吉', '阻', '塞', '斤', '驭', '畴', '默', '恰', '沟', '纠', '冲', '突', '碰', '检', '控', '售', '撤', '剖', '析', '晓', '奔', '波', '渊', '源', '亿', '域', '辟', '栏', '聘', '汲', '验', '袂', '厦', '冰', '『', '』', '福', '凌', '摄', '哈', '尔', '滨', '尼', '嘉', '皇', '魄', '洒', '雪', '搀', '厅', '溥', '忙', '姑', '矢', '寸', '贡', '架', '诣', '粉', '刮', '蜂', '蜜', '抹', '芬', '芳', '芜', '冻', '噩', '虔', '洗', '霜', '碾', '驱', '缤', '拢', '唇', '颊', '疤', '闪', '巴', '昏', '扎', '爬', '尸', '返', '扫', '糊', '迅', '速', '率', '掩', '渠', '瞎', '拖', '猪', '胶', '威', '牵', '蹒', '跚', '摧', '巫', '峡', '覆', '舟', '兽', '欺', '妥', '货', '钞', '贩', '搅', '逻', '舒', '顾', '倍', '诈', '勉', '亵', '渎', '肢', '鸭', '烤', '偷', '套', '锅', '柱', '捡', '标', '捞', '劝', '君', '惆', '怅', '婉', '眺', '董', '滩', '币', '湮', '沙', '矗', '烽', '晴', '朗', '丝', '绸', '隘', '芦', '岸', '柳', '胡', '泓', '牙', '骤', '弦', '仙', '橙', '沾', '晶', '莹', '刃', '纹', '腻', '拓', '塑', '吸', '翔', '惊', '蓝', '翱', '琵', '琶', '蹈', '碧', '牟', '朴', '淋', '漓', '娴', '魏', '隋', '雍', '肌', '综', '秦', '砂', '错', '阁', '银', '溪', '潺', '亘', '耸', '洲', '娱', '缀', '祁', '廊', '衰', '驼', '绎', '亚', '瓜', '郡', '枕', '咽', '柏', '疆', '吾', '阎', '霞', '豁', '船', '媳', '嫁', '粗', '秽', '丢', '睦', '愉', '悦', '秃', '亩', '梯', '柿', '桃', '栗', '杏', '梅', '棵', '贯', '池', '牡', '冬', '旷', '桥', '沥', '竣', '姐', '妹', '胳', '膊', '滴', '汗', '肆', '虐', '委', '邵', '懋', '癌', '赴', '淀', '燃', '腔', '淹', '罕', '储', '豪', '鸟', '佩', '巾', '跃', '朱', '棠', '梨', '槐', '椿', '透', '峪', '盎', '勃', '沸', '旬', '伍', '谅', '椅', '柜', '秩', '抬', '扩', '攘', '葡', '萄', '挫', '吨', '．', '顷', '冒', '劣', '滥', '酵', '脆', '档', '忠', '须', '竞', '坊', '瓶', '酗', '掀', '亡', '轿', '瑞', '俄', '澳', '寰', '沪', '械', '掌', '隧', '础', '软', '估', '廓', '轨', '允', '践', '磁', '网', '／', '讨', '圳', '赃', '配', '府', '摊', '租', '辽', '逛', '镇', '剂', '惠', '纺', '脑', '羞', '洽', '税', '麦', '赚', '渗', '赋', '萎', '鲁', '淘', '汰', '酌', '拣', '垃', '圾', '蝴', '涧', '惚', '僵', '扑', '祝', '葬', '桶', '翅', '毙', '爸', '捉', '坪', '蚁', '冶', '魂', '嗡', '嘤', '捕', '逗', '臾', '唤', '恬', '氤', '氲', '翼', '勒', '弧', '圈', '瞬', '潢', '宙', '拼', '凑', '翩', '烧', '朵', '熠', '歇', '逍', '炊', '粹', '笺', '骄', '喧', '嚣', '赫', '课', '涩', '聆', '孕', '猎', '襄', '吴', '庞', '冯', '庸', '闵', '祚', '庥', '苗', '讯', '钢', '挨', '靡', '玻', '璃', '橱', '苑', '畏', '逼', '迫', '薄', '甸', '拔', '萃', '框', '暮', '恋', '弯', '慌', '踢', '虎', '拘', '嘀', '咕', '厉', '毫', '铭', '夸', '奖', '寥', '猛', '劫', '夹', '倡', '漂', '吻', '慑', '阅', '盟', '援', '沃', '奈', '吁', '俘', '虏', '汹', '剩', '测', '撑', '驴', '伊', '豫', '缝', '痴', '狭', '抵', '御', '禄', '斥', '霸', '溺', '膨', '胀', '屁', '债', '爆', '叩', '－', '贺', '锥', '廷', '棘', '扭', '蒸', '败', '混', '淆', '帆', '钟', '吕', '硝', '淮', '役', '毋', '宁', '诠', '阐', '赣', '墩', '航', '徽', '巢', '瓢', '泥', '渡', '坂', '矶', '瓣', '怨', '枪', '烂', '渣', '峥', '砌', '彭', '聂', '臻', '澜', '柄', '剑', '竖', '坨', '佐', '殉', '吓', '荆', '茅', '勘', '崎', '岖', '擦', '巅', '鼻', '灿', '晰', '瞧', '矮', '呀', '陕', '皂', '壶', '葛', '暑', '璋', '棣', '挺', '齿', '凛', '丑', '睹', '戒', '奢', '勿', '翘', '鄙', '垢', '蓄', '檐', '栖', '棚', '谎', '萧', '瑟', '秧', '鸽', '蜀', '萨', '哗', '滔', '坑', '槛', '跪', '抛', '僚', '虫', '荫', '脂', '膏', '渔', '狗', '铜', '墟', '曹', '璀', '璨', '傻', '呕', '抄', '曝', '娘', '赛', '○', '键', '割', '垄', '搏', '讴', '躁', '哆', '咪', '惹', '烫', '抒', '甲', '乙', '锋', '莲', '垦', '植', '耕', '邱', '漏', '氏', '锄', '眠', '慢', '蔬', '垫', '畦', '卜', '帘', '狠', '湘', '湖', '堵', '订', '拳', '菱', '棉', '垠', '膜', '昕', '酉', '铸', '缴', '·', '窑', '蔡', '桩', '申', '仔', '贸', '眉', '骗', '肚', '皖', '陵', '谣', '芋', '旱', '柴', '盆', '菠', '茬', '枣', '葫', '贷', '粒', '匀', '稼', '啤', '烘', '苹', '屿', '莒', '弓', '籽', '滞', '笃', '邮', '拦', '谊', '俑', '挖', '睁', '亟', '鞭', '肺', '腑', '咸', '荤', '糖', '糕', '蛋', '盏', '汤', '拌', '泞', '芭', '鄂', '洼', '颠', '簸', '侄', '昌', '罚', '遵', '迭', '赖', '铮', '迁', '彰', '顽', '濡', '沫', '脏', '抉', '沂', '玲', '赊', '辰', '巩', '腿', '蜡', '拾', '躺', '雇', '襟', '纫', '衔', '坷', '寡', '沱', '砀', '扼', '筋', '酥', '芙', '蓉', '酬', '暇', '耘', '妆', '押', '湛', '饲', '筐', '喃', '箕', '靶', '颁', '署', '炒', '畸', '邪', '疯', '捆', '吆', '帷', '刁', '沛', '蚕', '瓦', '迥', '憋', '沈', '泳', '枢', '纽', '钉', '盼', '熄', '舶', '览', '瞩', '琦', '崛', '赁', '馈', '蜕', '‰', '蛙', '挽', '杜', '函', '饪', '罔', '荐', '携', '盐', '●', '棒', '莱', '穆', '熏', '黎', '斋', '踞', '黔', '辖', '驿', '芮', '倘', '鹊', '遴', '荧', '浙', '泠', '荟', '杭', '亥', '敏', '阜', '渍', '藉', '骑', '捧', '腹', '擅', '囚', '潇', '奸', '耍', '韧', '宰', '篮', '炙', '哄', '噪', '斐', '耿', '寿', '寺', '矜', '尴', '尬', '绣', '瓷', '泰', '恳', '霉', '蛀', '掣', '隅', '暂', '虞', '茏', '砸', '伐', '贮', '孜', '坝', '穗', '锯', '愣', '凳', '羔', '炕', '煞', '拴', '聊', '娶', '馋', '俺', '攒', '串', '趁', '粥', '碱', '瘦', '秆', '窘', '绵', '薯', '宪', '咐', '黝', '缸', '灶', '垒', '筷', '窿', '粪', '坯', '漆', '秸', '佝', '偻', '兮', '扉', '桐', '镶', '嵌', '黛', '讹', '冤', '臆', '婴', '迪', '汪', '拮', '胫', '瘤', '脊', '砚', '侈', '贿', '沭', '婪', '胃', '颤', '抖', '涣', '嫉', '妒', '赂', '刑', '弊', '琼', '∶', '刹', '芒', '霄', '漳', '擞', '樊', '鳖', '涨', '蟹', '乌', '寝', '泵', '涝', '遣', '樟', '楠', '盒', '募', '崭', '裤', '菊', '町', '俊', '掖', '瘫', '傍', '衫', '夷', '匪', '哉', '卓', '媒', '拆', '彤', '逃', '岳', '徕', '邯', '郸', '宅', '盗', '舌', '锡', '鹅', '屡', '惩', '禺', '乾', '贝', '茹', '魔', '亭', '昭', '熬', '珊', '泊', '昆', '靳', '呱', '坠', '锣', '靖', '挪', '簿', '徇', '杆', '菌', '焚', '堤', '踊', '蘑', '菇', '喘', '埠', '罐', '姚', '陉', '跻', '鑫', '俏', '睐', '愤', '禽', '链', '喂', '拱', '掺', '蹲', '磋', '侮', '斌', '菁', '炉', '愚', '陇', '尹', '薛', '岐', '洛', '梧', '凰', '禾', '渭', '讽', '伦', '浏', '烁', '澡', '粽', '蘸', '饼', '饺', '扳', '寐', '浃', '懈', '暨', '弘', '堰', '煤', '炭', '叨', '歹', '烦', '窃', '迟', '蛛', '钩', '苇', '萍', '枚', '缄', '踵', '翁', '谦', '巡', '曳', '萋', '衬', '巍', '趟', '碌', '锈', '铿', '锵', '棱', '岚', '攸', '俱', '淤', '啥', '饶', '兆', '稽', '臣', '谏', '堡', '匈', '虹', '熊', '猫', '拧', '壤', '迄', '埃', '翡', '釉', '媛', '钧', '卿', '曼', '廖', '猩', '浦', '坎', '砾', '筛', '肮', '讶', '踩', '蒂', '盯', '彬', '嗓', '秒', '跷', '菏', '卉', '兔', '溉', '汾', '礁', '秤', '恪', '蚊', '蝇', '裴', '厕', '汀', '沼', '雏', '坟', '韶', '啼', '毒', '勋', '痹', '惕', '竭', '厄', '舵', '嗅', '艘', '墅', '粘', '帜', '屠', '×', '幢', '哩', '腊', '棍', '撬', '庐', '妄', '孟', '窜', '噬', '瘠', '戎', '倥', '偬', '恤', '伫', '缅', '畹', '傣', '彝', '侨', '杠', '徘', '徊', '滇', '扇', '厘', '箭', '筒', '逞', '晃', '笼', '罩', '逆', '鼎', '辐', '鹏', '肿', '撂', '腺', '劈', '柬', '稚', '髓', '坤', '浸', '蚀', '骼', '绊', '骇', '脾', '砺', '歧', '曙', '碟', '拟', '髦', '滤', '鲇', '泯', '惰', '舱', '佼', '歉', '骚', '咬', '纲', '▲', '圭', '玛', '弗', '璞', '莘', '媲', '逮', '滕', '侠', '锏', '蒋', '旭', '贬', '瑛', '嘭', '泄', '钰', '艾', '狮', '琪', '敖', '澄', '哨', '淑', '胁', '龚', '咄', '炯', '帕', '黯', '韬', '晦', '潘', '叉', '珀', '冕', '凯', '莉', '鲍', '孰', '颓', '膝', '夭', '璇', '琳', '娜', '哽', '莎', '冉', '奠', '姆', '帅', '铲', '△', '薇', '倩', '汝', '咏', '橡', '硫', '骆', '烙', '兜', '扛', '驮', '柘', '隙', '呵', '朦', '胧', '梭', '辂', '呜', '酣', '孪', '哺', '扮', '扁', '褐', '珑', '拙', '纬', '荷', '搂', '舰', '艇', '潸', '韫', '匮', '乒', '乓', '寅', '啃', '榻', '斟', '闽', '袍', '铨', '殆', '跆', '跤', '券', '腕', '簧', '璐', '渝', '焉', '擂', '狱', '瀚', '醇', '霍', '埂', '氓', '匾', '烨', '磊', '郝', '晟', '昊', '螺', '壳', '弈', '雯', '阪', '讳', '诀', '炜', '魁', '粤', '兹', '蔓', '霖', '卸', '霾', '勰', '糙', '茎', '悯', '逶', '迤', '怠', '肠', '徙', '贱', '蛇', '旮', '旯', '崩', '峨', '甫', '沌', '谪', '宠', '仕', '贻', '曦', '姊', '嘈', '洱', '孵', '簇', '剔', '袄', '婿', '嗜', '蜚', '悼', '抑', '圃', '絮', '彗', '宸', '丙', '磺', '伺', '盔', '轴', '拭', '睿', '丕', '崔', '珲', '雁', '伞', '奎', '倪', '茨', '喀', '邢', '彦', '焊', '镐', '耙', '夙', '缆', '恃', '铝', '涤', '厌', '挠', '蹬', '氯', '氟', '烃', '雌', '胎', '兑', '溃', '忱', '癖', '钙', '尿', '椎', '铢', '镕', '烷', '钠', '俞', '拯', '溯', '瑰', '晒', '脖', '帐', '杉', '窥', '韦', '咖', '啡', '绑', '钮', '丫', '谴', '孚', '潞', '莠', '嬗', '骥', '涓', '俨', '闺', '芸', '谭', '叙', '冥', '盅', '诵', '吟', '沁', '蛮', '昧', '戮', '籁', '麟', '煎', '琐', '磅', '礴', '悸', '璧', '炬', '掰', '揉', '怯', '铃', '趴', '汕', '谜', '篷', '肾', '茸', '燥', '赘', '汛', '赡', '娩', '揣', '荔', '晕', '锹', '淄', '烯', '彪', '苎', '芹', '蔗', '锌', '磷', '肪', '溶', '碘', '禧', '纤', '咂', '辍', '篆', '玺', '囿', '锲', '祈', '宕', '莽', '赝', '鹤', '杼', '隽', '羁', '琅', '耆', '嘹', '犁', '陲', '侗', '珞', '粑', '椒', '熔', '缔', '詹', '坞', '仑', '痪', '掷', '捍', '赦', '缉', '〈', '〉', '氦', '硅', '芯', '梗', '枭', '碳', '闸', '娅', '朔', '氢', '缭', '楷', '锂', '锶', '锰', '酶', '猗', '茄', '钛', '鞍', '阀', '蒲', '觑', '猿', '吝', '祷', '棺', '镑', '唆', '睫', '渥', '煽', '敛', '殃', '垮', '侥', '倚', '姨', '崽', '钥', '匙', '嚷', '揪', '帼', '淫', '殴', '翌', '炖', '惦', '爹', '瑶', '柚', '勐', '佤', '葵', '珮', '陀', '匿', '肇', '缪', '濒', '爪', '蔑', '侦', '贼', '镀', '胱', '肽', '崴', '醛', '轧', '帚', '猴', '磕', '俾', '衅', '捂', '垣', '瑙', '皎', '阂', '玫', '怔', '酋', '猖', '绅', '忡', '飙', '衍', '瑚', '冗', '漯', '渤', '鸿', '飒', '缨', '炳', '膀', '膳', '胪', '茵', '偌', '呐', '枫', '狩', '湟', '氮', '獗', '榷', '谟', '佣', '雹', '鄱', '晾', '诰', '邹', '遨', '栓', '铆', '卞', '抨', '氨', '穴', '渚', '胰', '藻', '梳', '恣', '缮', '坍', '稷', '卤', '怆', '擎', '岱', '篝', '迸', '膺', '楹', '澎', '湃', '轼', '泾', '瘁', '漾', '宵', '甬', '眸', '馥', '楞', '兢', '郅', '庚', '鲭', '犀', '℃', '禹', '圻', '哇', '匡', '谍', '瑜', '缜', '捣', '翟', '熹', '罂', '粟', '枉', '琛', '阮', '蔼', '偕', '椰', '眷', '袤', '捅', '懵', '粼', '畲', '榄', '桔', '挝', '哮', '枇', '杷', '莆', '箫', '磬', '邑', '铧', '栾', '炽', '屎', '馁', '昀', '憧', '憬', '璜', '幡', '篡', '谬', '诬', '绞', '剿', '尉', '悖', '渲', '鹃', '岔', '睬', '乖', '皱', '骸', '摞', '燮', '瞥', '缳', '眯', '惮', '卒', '狐', '篪', '焘', '绥', '浚', '卅', '霓', '彷', '徨', '葆', '鹳', '唁', '祠', '帖', '潍', '镰', '皋', '殡', '焯', '甄', '祺', '噶', '驹', '羚', '搓', '箔', '涟', '漪', '痼', '裨', '犷', '麋', '笨', '卵', '屹', '辫', '谕', '褂', '棕', '嗯', '裙', '堕', '茁', '搪', '暧', '邸', '芗', '蝉', '碉', '荼', '赈', '筝', '忏', '饵', '诡', '裘', '琉', '绛', '涑', '袒', '柑', '桢', '祀', '瞰', '萦', '邝', '敷', '疟', '婷', '酱', '耪', '惶', '贞', '蕊', '褶', '庇', '咒', '瘟', '慵', '淇', '赭', '苞', '湍', '娓', '狡', '咧', '榴', '妮', '穹', '晏', '爵', '锚', '麾', '汨', '裳', '瑾', '捺', '窒', '垛', '苜', '蓿', '蠕', '蛟', '醺', '嗣', '奕', '删', '铉', '塬', '丐', '徜', '徉', '谧', '眶', '卦', '邬', '洌', '吮', '蹋', '钝', '肋', '枷', '螃', '骞', '槎', '筏', '鹭', '簌', '漱', '桓', '胄', '洙', '鲨', '锤', '郓', '拷', '锭', '飓', '趾', '褒', '灼', '笙', '胚', '栎', '铐', '涡', '沽', '淦', '戳', '陨', '铀', '鳌', '姥', '袜', '釜', '钕', '硼', '胺', '弋', '疹', '毯', '螂', '斡', '铱', '椭', '劾', '鸫', '讥', '叛', '诲', '懿', '羲', '霏', '砰', '稠', '疡', '殚', '谆', '祟', '浒', '茉', '辗', '樱', '眈', 'Ⅲ', '诋', '汴', '骧', '帧', '纂', '咎', '濑', '妊', '娠', '烬', '莨', '菪', '胥', '苯', '酚', '揆', '柩', '疙', '瘩', '菅', '憎', '岫', '戊', '戌', '滦', 'Ⅳ', '汞', 'Ⅴ', '鲅', '抠', '＋', '佃', '汐', '嘻', '鸥', '莺', '骝', '濮', '溴', '鏊', '沅', '袅', '闾', '宓', '翎', '泱', '<UNK>', '<PAD>'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (3905, 300))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if args.pretrain_embedding == 'random':\n",
    "#     embeddings = random_embedding(word2id, args.embedding_dim)\n",
    "# else:\n",
    "#     embedding_path = 'pretrain_embedding.npy'\n",
    "#     embeddings = np.array(np.load(embedding_path), dtype='float32')\n",
    "embeddings = random_embedding(word2id, 300)\n",
    "type(embeddings),embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24289457,  0.08744171, -0.19475493, -0.17992818, -0.05061282,\n",
       "        -0.05096334, -0.10729764,  0.19720572, -0.00817338, -0.02671687,\n",
       "        -0.21701367, -0.14643739,  0.10569254, -0.24897547, -0.24331471,\n",
       "         0.05571439,  0.068689  ,  0.03074452,  0.00349323, -0.19363198,\n",
       "         0.19154462,  0.08363545, -0.00707453, -0.03918197,  0.08813415,\n",
       "         0.06555788, -0.09478968,  0.04997296,  0.0188967 ,  0.18829729,\n",
       "        -0.17980649,  0.22334029, -0.05274172,  0.10673455, -0.06408237,\n",
       "         0.22457956,  0.03680328, -0.1484314 , -0.03141019,  0.05154869,\n",
       "        -0.12299992, -0.22925493, -0.05336275,  0.01276062, -0.18880174,\n",
       "         0.1003283 ,  0.0598359 , -0.08483516,  0.08332489,  0.1453224 ,\n",
       "         0.01927098, -0.20168571,  0.24328902,  0.21241413,  0.18247628,\n",
       "         0.07354892,  0.06478015,  0.06478026,  0.05219268, -0.02274675,\n",
       "         0.02637642,  0.07520191, -0.00105785, -0.02584241,  0.14295278,\n",
       "         0.17856646, -0.16664007,  0.14560308, -0.22622435, -0.1977466 ,\n",
       "        -0.14916117,  0.21117213, -0.06687596, -0.01387385, -0.21198702,\n",
       "         0.00835847, -0.22485395,  0.01151579, -0.17058977,  0.00429726,\n",
       "        -0.10214262,  0.13116953,  0.2287567 , -0.24376948, -0.11631561,\n",
       "        -0.19555831, -0.03227715, -0.00080919, -0.01267331,  0.21162952,\n",
       "        -0.03662066,  0.02518037,  0.12327513,  0.18229981,  0.05674255,\n",
       "         0.1131091 , -0.22406325,  0.14266954,  0.18703496,  0.12898019,\n",
       "         0.09725147, -0.22128399,  0.10897416,  0.20085569, -0.09428345,\n",
       "        -0.08461305, -0.24224594,  0.17098661,  0.05346867, -0.16587844,\n",
       "        -0.00594549, -0.1823333 , -0.20650895,  0.22559957, -0.15085869,\n",
       "         0.17416449,  0.24053927,  0.04426907,  0.17919469, -0.11174341,\n",
       "        -0.18123433,  0.03037961, -0.07979294, -0.19010492, -0.00073506,\n",
       "         0.00139577, -0.19318612,  0.20880578,  0.0304687 , -0.14366247,\n",
       "         0.1738064 , -0.03698132,  0.12106824,  0.00099098, -0.12418697,\n",
       "         0.18583986, -0.1838643 , -0.04911454, -0.09434474, -0.24738672,\n",
       "         0.12985803, -0.24809597, -0.22936222,  0.08809754,  0.07408525,\n",
       "        -0.21756004, -0.23159835, -0.05489233,  0.10815672,  0.14586417,\n",
       "        -0.13752328,  0.15091355, -0.16185299,  0.20394327,  0.15609312,\n",
       "        -0.1664396 ,  0.1389268 , -0.14564085, -0.07673976,  0.14285557,\n",
       "        -0.1762588 , -0.19163899, -0.0241354 , -0.05205753, -0.20361775,\n",
       "        -0.04695117, -0.17489865, -0.00181887,  0.15522899, -0.18527368,\n",
       "         0.08109821, -0.17218606,  0.12971489,  0.14865081,  0.23953754,\n",
       "         0.20178099, -0.14487858,  0.14923915,  0.03633904,  0.16360523,\n",
       "        -0.08399004, -0.07516418,  0.1911329 , -0.02863545, -0.21841817,\n",
       "         0.07011688, -0.02478515,  0.04775411, -0.08900914,  0.19528034,\n",
       "        -0.0704016 ,  0.07327671,  0.22995898, -0.07858352,  0.2004721 ,\n",
       "         0.05311187,  0.05149514,  0.21516278, -0.12297934, -0.03128913,\n",
       "         0.21855961,  0.04858401,  0.2088794 , -0.11449633, -0.17481358,\n",
       "         0.02533797, -0.11719415, -0.2432734 ,  0.12118447, -0.08877315,\n",
       "         0.11001884,  0.08301132, -0.00509894,  0.06793581, -0.06520788,\n",
       "         0.1735362 ,  0.01874172,  0.18866056, -0.0691931 , -0.19802661,\n",
       "        -0.07601918, -0.10822025,  0.01340855,  0.14581503, -0.02285989,\n",
       "        -0.05931222,  0.24650268, -0.1488309 , -0.15072791,  0.06685212,\n",
       "         0.11943785, -0.0478016 ,  0.07121542, -0.00865732, -0.22337282,\n",
       "         0.16980255,  0.14359342,  0.04775198, -0.21344894,  0.07266518,\n",
       "        -0.14824891,  0.00616609,  0.22506408, -0.05577096,  0.15452631,\n",
       "         0.12139945,  0.0782124 , -0.22134848, -0.17278649,  0.03325181,\n",
       "         0.08455919,  0.05460115,  0.12642449, -0.06300157,  0.16247942,\n",
       "         0.1745031 , -0.1562252 ,  0.09885005,  0.06004111,  0.05982185,\n",
       "         0.24979696, -0.08442345, -0.21576272,  0.05824175, -0.0136528 ,\n",
       "        -0.23604374, -0.2022964 ,  0.14082852,  0.1752053 ,  0.24437225,\n",
       "         0.15654184, -0.07999946, -0.23577116, -0.10235233, -0.23065116,\n",
       "         0.03164806, -0.01629915,  0.22643933, -0.08380176,  0.02479386,\n",
       "        -0.19785385,  0.12162909, -0.12539417, -0.06844547, -0.22657461,\n",
       "        -0.1767754 ,  0.20780046,  0.09603889, -0.09770753, -0.24719027,\n",
       "        -0.1143554 ,  0.20515914, -0.17285958,  0.13778242,  0.06615926,\n",
       "        -0.22448982,  0.14278169, -0.05309493,  0.10512494, -0.24242361]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[:1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4631,\n",
       " [(['中',\n",
       "    '共',\n",
       "    '中',\n",
       "    '央',\n",
       "    '致',\n",
       "    '中',\n",
       "    '国',\n",
       "    '致',\n",
       "    '公',\n",
       "    '党',\n",
       "    '十',\n",
       "    '一',\n",
       "    '大',\n",
       "    '的',\n",
       "    '贺',\n",
       "    '词'],\n",
       "   ['B-ORG',\n",
       "    'I-ORG',\n",
       "    'I-ORG',\n",
       "    'I-ORG',\n",
       "    'O',\n",
       "    'B-ORG',\n",
       "    'I-ORG',\n",
       "    'I-ORG',\n",
       "    'I-ORG',\n",
       "    'I-ORG',\n",
       "    'I-ORG',\n",
       "    'I-ORG',\n",
       "    'I-ORG',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O']),\n",
       "  (['各', '位', '代', '表', '、', '各', '位', '同', '志', '：'],\n",
       "   ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## read corpus and get training data\n",
    "# if args.mode != 'demo':\n",
    "#     train_path = os.path.join('.', args.train_data, 'train_data')\n",
    "#     test_path = os.path.join('.', args.test_data, 'test_data')\n",
    "#     train_data = read_corpus(train_path)\n",
    "#     test_data = read_corpus(test_path)\n",
    "#     test_size = len(test_data)\n",
    "train_path = os.path.join('.', 'data_path', 'train_data')\n",
    "test_path = os.path.join('.', 'data_path', 'test_data')\n",
    "train_data = read_corpus(train_path)\n",
    "test_data = read_corpus(test_path)\n",
    "test_size = len(test_data)\n",
    "len(test_data),test_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BiLSTM-CRF for Chinese NER task\n"
     ]
    }
   ],
   "source": [
    "## paths setting\n",
    "paths = {}\n",
    "# timestamp = str(int(time.time())) if args.mode == 'train' else args.demo_model\n",
    "timestamp = str(int(time.time()))\n",
    "output_path = os.path.join('.', 'data_path'+\"_save\", timestamp)\n",
    "if not os.path.exists(output_path): os.makedirs(output_path)\n",
    "summary_path = os.path.join(output_path, \"summaries\")\n",
    "paths['summary_path'] = summary_path\n",
    "if not os.path.exists(summary_path): os.makedirs(summary_path)\n",
    "model_path = os.path.join(output_path, \"checkpoints/\")\n",
    "if not os.path.exists(model_path): os.makedirs(model_path)\n",
    "ckpt_prefix = os.path.join(model_path, \"model\")\n",
    "paths['model_path'] = ckpt_prefix\n",
    "result_path = os.path.join(output_path, \"results\")\n",
    "paths['result_path'] = result_path\n",
    "if not os.path.exists(result_path): os.makedirs(result_path)\n",
    "log_path = os.path.join(result_path, \"log.txt\")\n",
    "paths['log_path'] = log_path\n",
    "# get_logger(log_path).info(str(args))\n",
    "get_logger(log_path).info('BiLSTM-CRF for Chinese NER task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 50658\n",
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 16:10:10 epoch 1, step 1, loss: 81.3, global_step: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 16:10:10 epoch 1, step 300, loss: 6.431, global_step: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 16:10:10 epoch 1, step 600, loss: 4.385, global_step: 600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 16:10:10 epoch 1, step 792, loss: 4.233, global_step: 792\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 16:36:43 epoch 2, step 1, loss: 3.818, global_step: 793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 16:36:43 epoch 2, step 300, loss: 3.629, global_step: 1092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 16:36:43 epoch 2, step 600, loss: 2.615, global_step: 1392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 16:36:43 epoch 2, step 792, loss: 1.123, global_step: 1584\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 17:02:59 epoch 3, step 1, loss: 1.99, global_step: 1585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 17:02:59 epoch 3, step 300, loss: 1.451, global_step: 1884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 17:02:59 epoch 3, step 600, loss: 1.427, global_step: 2184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 17:02:59 epoch 3, step 792, loss: 1.363, global_step: 2376\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 17:29:10 epoch 4, step 1, loss: 1.187, global_step: 2377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 17:29:10 epoch 4, step 300, loss: 2.201, global_step: 2676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 17:29:10 epoch 4, step 600, loss: 1.487, global_step: 2976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 17:29:10 epoch 4, step 792, loss: 1.234, global_step: 3168\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 17:55:23 epoch 5, step 1, loss: 1.007, global_step: 3169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 17:55:23 epoch 5, step 300, loss: 0.9421, global_step: 3468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 17:55:23 epoch 5, step 600, loss: 0.8956, global_step: 3768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 17:55:23 epoch 5, step 792, loss: 0.4034, global_step: 3960\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 18:21:50 epoch 6, step 1, loss: 0.7763, global_step: 3961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 18:21:50 epoch 6, step 300, loss: 1.124, global_step: 4260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 18:21:50 epoch 6, step 600, loss: 1.373, global_step: 4560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 18:21:50 epoch 6, step 792, loss: 1.146, global_step: 4752\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 18:47:45 epoch 7, step 1, loss: 1.111, global_step: 4753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 18:47:45 epoch 7, step 300, loss: 0.7679, global_step: 5052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 18:47:45 epoch 7, step 600, loss: 1.123, global_step: 5352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 18:47:45 epoch 7, step 792, loss: 0.4533, global_step: 5544\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 19:13:25 epoch 8, step 1, loss: 0.566, global_step: 5545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 19:13:25 epoch 8, step 300, loss: 0.5264, global_step: 5844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 19:13:25 epoch 8, step 600, loss: 0.3118, global_step: 6144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 19:13:25 epoch 8, step 792, loss: 0.4091, global_step: 6336\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 19:39:06 epoch 9, step 1, loss: 0.8978, global_step: 6337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 19:39:06 epoch 9, step 300, loss: 0.4508, global_step: 6636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 19:39:06 epoch 9, step 600, loss: 0.658, global_step: 6936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 19:39:06 epoch 9, step 792, loss: 0.6943, global_step: 7128\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 20:04:45 epoch 10, step 1, loss: 0.3697, global_step: 7129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 20:04:45 epoch 10, step 300, loss: 0.5199, global_step: 7428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 20:04:45 epoch 10, step 600, loss: 0.7356, global_step: 7728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 20:04:45 epoch 10, step 792, loss: 0.495, global_step: 7920\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 20:30:25 epoch 11, step 1, loss: 0.2465, global_step: 7921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 20:30:25 epoch 11, step 300, loss: 0.4629, global_step: 8220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 20:30:25 epoch 11, step 600, loss: 0.4688, global_step: 8520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 20:30:25 epoch 11, step 792, loss: 0.2265, global_step: 8712\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 20:56:04 epoch 12, step 1, loss: 0.5948, global_step: 8713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 20:56:04 epoch 12, step 300, loss: 0.4058, global_step: 9012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 20:56:04 epoch 12, step 600, loss: 0.6562, global_step: 9312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 20:56:04 epoch 12, step 792, loss: 0.443, global_step: 9504\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 21:21:46 epoch 13, step 1, loss: 0.3217, global_step: 9505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 21:21:46 epoch 13, step 300, loss: 0.3807, global_step: 9804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 21:21:46 epoch 13, step 600, loss: 0.5521, global_step: 10104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 21:21:46 epoch 13, step 792, loss: 0.5192, global_step: 10296\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 21:47:37 epoch 14, step 1, loss: 0.3458, global_step: 10297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 21:47:37 epoch 14, step 300, loss: 0.3977, global_step: 10596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 21:47:37 epoch 14, step 600, loss: 0.2574, global_step: 10896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 21:47:37 epoch 14, step 792, loss: 0.1613, global_step: 11088\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 22:13:18 epoch 15, step 1, loss: 0.3259, global_step: 11089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 22:13:18 epoch 15, step 300, loss: 0.321, global_step: 11388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 22:13:18 epoch 15, step 600, loss: 0.1854, global_step: 11688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 22:13:18 epoch 15, step 792, loss: 0.5081, global_step: 11880\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 22:38:59 epoch 16, step 1, loss: 0.1505, global_step: 11881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 22:38:59 epoch 16, step 300, loss: 0.372, global_step: 12180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 22:38:59 epoch 16, step 600, loss: 0.4547, global_step: 12480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 22:38:59 epoch 16, step 792, loss: 0.3029, global_step: 12672\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 23:04:34 epoch 17, step 1, loss: 0.2202, global_step: 12673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 23:04:34 epoch 17, step 300, loss: 0.2861, global_step: 12972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 23:04:34 epoch 17, step 600, loss: 0.2432, global_step: 13272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 23:04:34 epoch 17, step 792, loss: 0.3274, global_step: 13464\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 23:30:08 epoch 18, step 1, loss: 0.1547, global_step: 13465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 23:30:08 epoch 18, step 300, loss: 0.1879, global_step: 13764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 23:30:08 epoch 18, step 600, loss: 0.1327, global_step: 14064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 23:30:08 epoch 18, step 792, loss: 0.3012, global_step: 14256\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 23:55:44 epoch 19, step 1, loss: 0.2128, global_step: 14257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 23:55:44 epoch 19, step 300, loss: 0.3526, global_step: 14556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 23:55:44 epoch 19, step 600, loss: 0.2853, global_step: 14856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-21 23:55:44 epoch 19, step 792, loss: 0.3318, global_step: 15048\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 00:21:20 epoch 20, step 1, loss: 0.1698, global_step: 15049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 00:21:20 epoch 20, step 300, loss: 0.4153, global_step: 15348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 00:21:20 epoch 20, step 600, loss: 0.1047, global_step: 15648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 00:21:20 epoch 20, step 792, loss: 0.2517, global_step: 15840\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 00:46:57 epoch 21, step 1, loss: 0.1846, global_step: 15841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 00:46:57 epoch 21, step 300, loss: 0.2054, global_step: 16140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 00:46:57 epoch 21, step 600, loss: 0.06886, global_step: 16440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 00:46:57 epoch 21, step 792, loss: 0.01674, global_step: 16632\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 01:12:30 epoch 22, step 1, loss: 0.1167, global_step: 16633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 01:12:30 epoch 22, step 300, loss: 0.2216, global_step: 16932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 01:12:30 epoch 22, step 600, loss: 0.1585, global_step: 17232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 01:12:30 epoch 22, step 792, loss: 0.1607, global_step: 17424\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 01:38:05 epoch 23, step 1, loss: 0.122, global_step: 17425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 01:38:05 epoch 23, step 300, loss: 0.1728, global_step: 17724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 01:38:05 epoch 23, step 600, loss: 0.2471, global_step: 18024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 01:38:05 epoch 23, step 792, loss: 0.2532, global_step: 18216\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 02:03:35 epoch 24, step 1, loss: 0.1216, global_step: 18217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 02:03:35 epoch 24, step 300, loss: 0.1714, global_step: 18516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 02:03:35 epoch 24, step 600, loss: 0.1439, global_step: 18816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 02:03:35 epoch 24, step 792, loss: 0.1225, global_step: 19008\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 02:29:09 epoch 25, step 1, loss: 0.184, global_step: 19009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 02:29:09 epoch 25, step 300, loss: 0.05809, global_step: 19308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 02:29:09 epoch 25, step 600, loss: 0.1366, global_step: 19608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 02:29:09 epoch 25, step 792, loss: 0.2953, global_step: 19800\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 02:54:44 epoch 26, step 1, loss: 0.2714, global_step: 19801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 02:54:44 epoch 26, step 300, loss: 0.1173, global_step: 20100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 02:54:44 epoch 26, step 600, loss: 0.07839, global_step: 20400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 02:54:44 epoch 26, step 792, loss: 0.1402, global_step: 20592\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 03:20:19 epoch 27, step 1, loss: 0.1319, global_step: 20593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 03:20:19 epoch 27, step 300, loss: 0.2102, global_step: 20892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 03:20:19 epoch 27, step 600, loss: 0.1237, global_step: 21192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 03:20:19 epoch 27, step 792, loss: 0.1871, global_step: 21384\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 03:45:58 epoch 28, step 1, loss: 0.09796, global_step: 21385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 03:45:58 epoch 28, step 300, loss: 0.1713, global_step: 21684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 03:45:58 epoch 28, step 600, loss: 0.1261, global_step: 21984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 03:45:58 epoch 28, step 792, loss: 0.05246, global_step: 22176\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 04:11:34 epoch 29, step 1, loss: 0.1422, global_step: 22177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 04:11:34 epoch 29, step 300, loss: 0.114, global_step: 22476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 04:11:34 epoch 29, step 600, loss: 0.06077, global_step: 22776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 04:11:34 epoch 29, step 792, loss: 0.08432, global_step: 22968\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 04:37:08 epoch 30, step 1, loss: 0.1753, global_step: 22969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 04:37:08 epoch 30, step 300, loss: 0.1175, global_step: 23268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 04:37:08 epoch 30, step 600, loss: 0.1582, global_step: 23568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 04:37:08 epoch 30, step 792, loss: 0.1159, global_step: 23760\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 05:02:45 epoch 31, step 1, loss: 0.03982, global_step: 23761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 05:02:45 epoch 31, step 300, loss: 0.05015, global_step: 24060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 05:02:45 epoch 31, step 600, loss: 0.06156, global_step: 24360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 05:02:45 epoch 31, step 792, loss: 0.03912, global_step: 24552\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 05:28:18 epoch 32, step 1, loss: 0.1318, global_step: 24553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 05:28:18 epoch 32, step 300, loss: 0.2084, global_step: 24852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 05:28:18 epoch 32, step 600, loss: 0.07248, global_step: 25152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 05:28:18 epoch 32, step 792, loss: 0.1994, global_step: 25344\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 05:53:57 epoch 33, step 1, loss: 0.0961, global_step: 25345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 05:53:57 epoch 33, step 300, loss: 0.263, global_step: 25644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 05:53:57 epoch 33, step 600, loss: 0.05109, global_step: 25944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 05:53:57 epoch 33, step 792, loss: 0.08052, global_step: 26136\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 06:19:31 epoch 34, step 1, loss: 0.271, global_step: 26137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 06:19:31 epoch 34, step 300, loss: 0.06651, global_step: 26436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 06:19:31 epoch 34, step 600, loss: 0.1301, global_step: 26736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 06:19:31 epoch 34, step 792, loss: 0.4094, global_step: 26928\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 06:45:07 epoch 35, step 1, loss: 0.1873, global_step: 26929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 06:45:07 epoch 35, step 300, loss: 0.05622, global_step: 27228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 06:45:07 epoch 35, step 600, loss: 0.08634, global_step: 27528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 06:45:07 epoch 35, step 792, loss: 0.09345, global_step: 27720\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 07:10:40 epoch 36, step 1, loss: 0.1686, global_step: 27721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 07:10:40 epoch 36, step 300, loss: 0.07622, global_step: 28020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 07:10:40 epoch 36, step 600, loss: 0.2529, global_step: 28320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 07:10:40 epoch 36, step 792, loss: 0.001877, global_step: 28512\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 07:36:18 epoch 37, step 1, loss: 0.1564, global_step: 28513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 07:36:18 epoch 37, step 300, loss: 0.1176, global_step: 28812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 07:36:18 epoch 37, step 600, loss: 0.04503, global_step: 29112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 07:36:18 epoch 37, step 792, loss: 0.09765, global_step: 29304\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 08:01:52 epoch 38, step 1, loss: 0.2004, global_step: 29305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 08:01:52 epoch 38, step 300, loss: 0.1441, global_step: 29604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 08:01:52 epoch 38, step 600, loss: 0.03515, global_step: 29904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 08:01:52 epoch 38, step 792, loss: 0.05515, global_step: 30096\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 08:28:22 epoch 39, step 1, loss: 0.08572, global_step: 30097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 08:28:22 epoch 39, step 300, loss: 0.1411, global_step: 30396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 08:28:22 epoch 39, step 600, loss: 0.1432, global_step: 30696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 08:28:22 epoch 39, step 792, loss: 0.08669, global_step: 30888\n",
      "===========validation / test===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 08:54:37 epoch 40, step 1, loss: 0.09028, global_step: 30889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 300 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 08:54:37 epoch 40, step 300, loss: 0.06054, global_step: 31188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 600 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 08:54:37 epoch 40, step 600, loss: 0.04246, global_step: 31488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 792 batch / 792 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-22 08:54:37 epoch 40, step 792, loss: 0.02431, global_step: 31680\n",
      "===========validation / test===========\n"
     ]
    }
   ],
   "source": [
    "## training model\n",
    "# if args.mode == 'train':\n",
    "#     model = BiLSTM_CRF(args, embeddings, tag2label, word2id, paths, config=config)\n",
    "#     model.build_graph()\n",
    "\n",
    "#     ## hyperparameters-tuning, split train/dev\n",
    "#     # dev_data = train_data[:5000]; dev_size = len(dev_data)\n",
    "#     # train_data = train_data[5000:]; train_size = len(train_data)\n",
    "#     # print(\"train data: {0}\\ndev data: {1}\".format(train_size, dev_size))\n",
    "#     # model.train(train=train_data, dev=dev_data)\n",
    "\n",
    "#     ## train model on the whole training data\n",
    "#     print(\"train data: {}\".format(len(train_data)))\n",
    "#     model.train(train=train_data, dev=test_data)  # use test_data as the dev_data to see overfitting phenomena\n",
    "\n",
    "\n",
    "# model = BiLSTM_CRF(args, embeddings, tag2label, word2id, paths, config=config)\n",
    "model = BiLSTM_CRF(embeddings, tag2label, word2id, paths=paths, config=config)\n",
    "model.build_graph()\n",
    "\n",
    "## hyperparameters-tuning, split train/dev\n",
    "# dev_data = train_data[:5000]; dev_size = len(dev_data)\n",
    "# train_data = train_data[5000:]; train_size = len(train_data)\n",
    "# print(\"train data: {0}\\ndev data: {1}\".format(train_size, dev_size))\n",
    "# model.train(train=train_data, dev=dev_data)\n",
    "\n",
    "## train model on the whole training data\n",
    "print(\"train data: {}\".format(len(train_data)))\n",
    "model.train(train=train_data, dev=test_data)  # use test_data as the dev_data to see overfitting phenomena\n",
    "# tf.nn.embedding_lookup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=========== testing ===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data_path_save\\1526890202\\checkpoints/model-31680\n",
      "test data: 4631\n",
      "INFO:tensorflow:Restoring parameters from .\\data_path_save\\1526890202\\checkpoints/model-31680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from .\\data_path_save\\1526890202\\checkpoints/model-31680\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Key words_2/_word_embeddings not found in checkpoint\n\t [[Node: save_1/RestoreV2_28 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_28/tensor_names, save_1/RestoreV2_28/shape_and_slices)]]\n\nCaused by op 'save_1/RestoreV2_28', defined at:\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-7966aea2f916>\", line 17, in <module>\n    model.test(test_data)\n  File \"D:\\github\\zh-NER-TF\\model.py\", line 175, in test\n    saver = tf.train.Saver()\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1218, in __init__\n    self.build()\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1227, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1263, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 751, in _build_internal\n    restore_sequentially, reshape)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 427, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 267, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1020, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key words_2/_word_embeddings not found in checkpoint\n\t [[Node: save_1/RestoreV2_28 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_28/tensor_names, save_1/RestoreV2_28/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Key words_2/_word_embeddings not found in checkpoint\n\t [[Node: save_1/RestoreV2_28 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_28/tensor_names, save_1/RestoreV2_28/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7966aea2f916>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# model.build_graph()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test data: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\github\\zh-NER-TF\\model.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, test)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'=========== testing ==========='\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m             \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m             \u001b[0mlabel_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdev_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1664\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1665\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1666\u001b[1;33m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1667\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_eager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Key words_2/_word_embeddings not found in checkpoint\n\t [[Node: save_1/RestoreV2_28 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_28/tensor_names, save_1/RestoreV2_28/shape_and_slices)]]\n\nCaused by op 'save_1/RestoreV2_28', defined at:\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-7966aea2f916>\", line 17, in <module>\n    model.test(test_data)\n  File \"D:\\github\\zh-NER-TF\\model.py\", line 175, in test\n    saver = tf.train.Saver()\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1218, in __init__\n    self.build()\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1227, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1263, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 751, in _build_internal\n    restore_sequentially, reshape)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 427, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 267, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1020, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key words_2/_word_embeddings not found in checkpoint\n\t [[Node: save_1/RestoreV2_28 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_28/tensor_names, save_1/RestoreV2_28/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "## testing model\n",
    "# elif args.mode == 'test':\n",
    "#     ckpt_file = tf.train.latest_checkpoint(model_path)\n",
    "#     print(ckpt_file)\n",
    "#     paths['model_path'] = ckpt_file\n",
    "#     model = BiLSTM_CRF(args, embeddings, tag2label, word2id, paths, config=config)\n",
    "#     model.build_graph()\n",
    "#     print(\"test data: {}\".format(test_size))\n",
    "#     model.test(test_data)\n",
    "## testing model\n",
    "ckpt_file = tf.train.latest_checkpoint(model_path)\n",
    "print(ckpt_file)\n",
    "paths['model_path'] = ckpt_file\n",
    "# model = BiLSTM_CRF(embeddings, tag2label, word2id, paths, config=config)\n",
    "# model.build_graph()\n",
    "print(\"test data: {}\".format(test_size))\n",
    "model.test(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data_path_save\\1526890202\\checkpoints/model-31680\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable bi-lstm/bidirectional_rnn/fw/lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9e0de5b9674b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_path'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mckpt_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBiLSTM_CRF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag2label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\github\\zh-NER-TF\\model.py\u001b[0m in \u001b[0;36mbuild_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_placeholders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup_layer_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiLSTM_layer_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_pred_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\github\\zh-NER-TF\\model.py\u001b[0m in \u001b[0;36mbiLSTM_layer_op\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m                 \u001b[0msequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m                 dtype=tf.float32)\n\u001b[0m\u001b[0;32m     83\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput_fw_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_bw_seq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_pl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mbidirectional_dynamic_rnn\u001b[1;34m(cell_fw, cell_bw, inputs, sequence_length, initial_state_fw, initial_state_bw, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[0;32m    399\u001b[0m           \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_state_fw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m           \u001b[0mparallel_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m           time_major=time_major, scope=fw_scope)\n\u001b[0m\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[1;31m# Backward direction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[1;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m         dtype=dtype)\n\u001b[0m\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[1;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[0;32m    775\u001b[0m       \u001b[0mloop_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m       swap_memory=swap_memory)\n\u001b[0m\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m   \u001b[1;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\u001b[0m\n\u001b[0;32m   2814\u001b[0m     \u001b[0mloop_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWhileContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=redefined-outer-name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2815\u001b[0m     \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2816\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2817\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[1;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2638\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2639\u001b[0m       original_body_result, exit_vars = self._BuildLoop(\n\u001b[1;32m-> 2640\u001b[1;33m           pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[0;32m   2641\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2642\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[1;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2588\u001b[0m         \u001b[0mstructure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moriginal_loop_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2589\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[1;32m-> 2590\u001b[1;33m     \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2591\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m       \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[1;34m(time, output_ta_t, state)\u001b[0m\n\u001b[0;32m    758\u001b[0m           \u001b[0mcall_cell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcall_cell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m           \u001b[0mstate_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m           skip_conditionals=True)\n\u001b[0m\u001b[0;32m    761\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m       \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_rnn_step\u001b[1;34m(time, sequence_length, min_sequence_length, max_sequence_length, zero_output, state, call_cell, state_size, skip_conditionals)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;31m# steps.  This is faster when max_seq_len is equal to the number of unrolls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;31m# (which is typical for dynamic_rnn).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m     \u001b[0mnew_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m     \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mnew_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m     \u001b[0mcall_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, state, scope)\u001b[0m\n\u001b[0;32m    181\u001b[0m       with vs.variable_scope(vs.get_variable_scope(),\n\u001b[0;32m    182\u001b[0m                              custom_getter=self._rnn_get_variable):\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNNCell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0min_graph_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, state)\u001b[0m\n\u001b[0;32m    606\u001b[0m               partitioned_variables.fixed_size_partitioner(\n\u001b[0;32m    607\u001b[0m                   self._num_unit_shards))\n\u001b[1;32m--> 608\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_linear1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Linear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_prev\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[1;31m# i = input_gate, j = new_input, f = forget_gate, o = output_gate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, output_size, build_bias, bias_initializer, kernel_initializer)\u001b[0m\n\u001b[0;32m   1169\u001b[0m           \u001b[0m_WEIGHTS_VARIABLE_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtotal_arg_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1171\u001b[1;33m           initializer=kernel_initializer)\n\u001b[0m\u001b[0;32m   1172\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mbuild_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouter_scope\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minner_scope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1201\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m       constraint=constraint)\n\u001b[0m\u001b[0;32m   1204\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1205\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1090\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1092\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m   1093\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m    415\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;34m\"constraint\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mestimator_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[0mcustom_getter_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"constraint\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mcustom_getter_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m       return _true_getter(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m_rnn_get_variable\u001b[1;34m(self, getter, *args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m     \u001b[0mvariable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m       trainable = (variable in tf_variables.trainable_variables() or\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    740\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 742\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    743\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable bi-lstm/bidirectional_rnn/fw/lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "## demo\n",
    "# elif args.mode == 'demo':\n",
    "#     ckpt_file = tf.train.latest_checkpoint(model_path)\n",
    "#     print(ckpt_file)\n",
    "#     paths['model_path'] = ckpt_file\n",
    "#     model = BiLSTM_CRF(args, embeddings, tag2label, word2id, paths, config=config)\n",
    "#     model.build_graph()\n",
    "#     saver = tf.train.Saver()\n",
    "#     with tf.Session(config=config) as sess:\n",
    "#         print('============= demo =============')\n",
    "#         saver.restore(sess, ckpt_file)\n",
    "#         while(1):\n",
    "#             print('Please input your sentence:')\n",
    "#             demo_sent = input()\n",
    "#             if demo_sent == '' or demo_sent.isspace():\n",
    "#                 print('See you next time!')\n",
    "#                 break\n",
    "#             else:\n",
    "#                 demo_sent = list(demo_sent.strip())\n",
    "#                 demo_data = [(demo_sent, ['O'] * len(demo_sent))]\n",
    "#                 tag = model.demo_one(sess, demo_data)\n",
    "#                 PER, LOC, ORG = get_entity(tag, demo_sent)\n",
    "#                 print('PER: {}\\nLOC: {}\\nORG: {}'.format(PER, LOC, ORG))\n",
    "ckpt_file = tf.train.latest_checkpoint(model_path)\n",
    "print(ckpt_file)\n",
    "paths['model_path'] = ckpt_file\n",
    "model = BiLSTM_CRF(embeddings, tag2label, word2id, paths, config=config)\n",
    "model.build_graph()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session(config=config) as sess:\n",
    "    print('============= demo =============')\n",
    "    saver.restore(sess, ckpt_file)\n",
    "    while(1):\n",
    "        print('Please input your sentence:')\n",
    "        demo_sent = input()\n",
    "        if demo_sent == '' or demo_sent.isspace():\n",
    "            print('See you next time!')\n",
    "            break\n",
    "        else:\n",
    "            demo_sent = list(demo_sent.strip())\n",
    "            demo_data = [(demo_sent, ['O'] * len(demo_sent))]\n",
    "            tag = model.demo_one(sess, demo_data)\n",
    "            PER, LOC, ORG = get_entity(tag, demo_sent)\n",
    "            print('PER: {}\\nLOC: {}\\nORG: {}'.format(PER, LOC, ORG))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pretrain_embedding.npy',model.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
